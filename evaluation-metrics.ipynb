{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Object Detection\n",
    "\n",
    "## Average Precision\n",
    "\n",
    "> The implementation of Average Precision is mainly based on the 11-point Interpolated Average Precision that is proposed in the book 'Introduction to Information Retrieval', https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html. \n",
    "\n",
    "The implementation of the Average Precision here is adapted from the 101 Point Interpolation AP evaluation metric that is introduced by MS COCO in 2014.\n",
    "- AP is calculated at 101 points rather than 11 points, on ranging recall values from 0 to 1 at a step of 0.01 \n",
    "- AP is calculated for a set of 10 different IoU thresholds and then averaged. The IoU threshold ranges from 0.5 to 0.95 at a step frequency of 0.05\n",
    "\n",
    "## Average Recall\n",
    "\n",
    "> The implementation of Average Recall is mainly adapted from the paper 'What makes for effective detection proposals?' https://arxiv.org/pdf/1502.05082.pdf. \n",
    "- AR is calculated as 2 times area under the recall-iou curve\n",
    "- Recall is calculated for 5 different IoU thresholds starting from 0.5 to 1 at a step frequency of 0.1\n",
    "\n",
    "---\n",
    "\n",
    "Some of the considerations for the evaluation is explained below:\n",
    "- Unmatched detecting bounding boxes are considered FP\n",
    "- Matched detecting bounding boxes with IoU overlap greater than or equal to IoU threshold are considered TP \n",
    "- **Precision** is calculated as TP / (TP + FP)\n",
    "- TP + FP is calculated as the number of predicted bounding boxes\n",
    "- **Recall** is calculated as TP / (TP + FN)\n",
    "- TP + FN is considered as the total number of ground truth bounding boxes for dataset\n",
    "- Detected bounding boxes with confidence score less than score threshold are not included in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1uhOOvFMIMl"
   },
   "outputs": [],
   "source": [
    "def calc_iou(ground_truth_bbox, pred_bbox):\n",
    "    x1, y1, x2, y2 = ground_truth_bbox\n",
    "    p_x1, p_y1, p_x2, p_y2 = pred_bbox\n",
    "    \n",
    "    # determine possible intersection points\n",
    "    left_max = x1 if x1 > p_x1 else p_x1\n",
    "    right_min = x2 if p_x2 > x2 else p_x2\n",
    "    top_max = y1 if y1 > p_y1 else p_y1\n",
    "    bottom_min = y2 if p_y2 > y2 else p_y2\n",
    "    \n",
    "    # return 0 if bounding boxes do not intersect \n",
    "    if left_max > right_min or top_max > bottom_min:\n",
    "        return 0.0\n",
    "    \n",
    "    # compute width and height of overlapping area\n",
    "    overlap_x = right_min - left_max\n",
    "    overlap_y = bottom_min - top_max\n",
    "\n",
    "    # compute intersection area       \n",
    "    intersect_area = overlap_x * overlap_y\n",
    "    \n",
    "    # compute union area = area of gt_bbox + area of pred_bbox - intersection area\n",
    "    w1, h1, w2, h2 = x2 - x1, y2 - y1, p_x2 - p_x1, p_y2 - p_y1  \n",
    "    union_area = (w1 * h1) + (w2 * h2) - intersect_area\n",
    "    \n",
    "    # compute iou = intersection area / area of union\n",
    "    iou = intersect_area / float(union_area)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVzayuhQNhoM"
   },
   "outputs": [],
   "source": [
    "# A detected BB (BBdt) and a ground truth BB (BBgt)\n",
    "# form a potential match if they overlap sufficiently (iou >= .5)\n",
    "\n",
    "# each BBdt and BBgt may be matched at most once\n",
    "# detections with highest confidence are matched first\n",
    "\n",
    "# match detected bounding boxes with ground-truth bounding boxes \n",
    "def get_iou_of_matched_bbox(gt_bbox_list, dt_bbox_list, iou_threshold = 0.5):\n",
    "    \n",
    "    len_dt_bbox = dt_bbox_list.shape[0]\n",
    "    len_gt_bbox = gt_bbox_list.shape[0]\n",
    "    \n",
    "    if len_dt_bbox == 0:\n",
    "        return []\n",
    "    \n",
    "    iou_matrix = np.zeros((len_dt_bbox, len_gt_bbox))\n",
    "    \n",
    "    # for each detection, match with highest overlap  \n",
    "    matched_bboxes = []\n",
    "    used_ind = []\n",
    "    for i, dt_bbox in enumerate(dt_bbox_list):   # detections are already sorted by confidence scores\n",
    "        for j, gt_bbox in enumerate(gt_bbox_list):\n",
    "            if not j in used_ind:   # if ground-truth bbox is available\n",
    "                \n",
    "                # calculate iou\n",
    "                iou_matrix[i][j] = calc_iou(gt_bbox, dt_bbox)\n",
    "        \n",
    "        # get the highest overlap\n",
    "        idx = np.argmax(iou_matrix[i,:])\n",
    "        \n",
    "        # check if bounding boxes form a potential match (iou >= .5)\n",
    "        if iou_matrix[i][idx] >= iou_threshold:\n",
    "            # add the matched bboxes indices with iou\n",
    "            matched_bboxes.append([i, idx, iou_matrix[i][idx]])\n",
    "            # remove gorund-truth bbox from the matrix, set availability\n",
    "            used_ind.append(idx)\n",
    "            \n",
    "    return np.array(matched_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMgHZ2LiPuAL"
   },
   "outputs": [],
   "source": [
    "# Average Precision: 101 point interpolation AP of precision-recall curve\n",
    "def calc_average_precision(eval_table, iou_thresholds, pred_scores, total_num_gt_bbox):\n",
    "    \n",
    "    # init a list of average precision values for different iou thresholds\n",
    "    ap_vals = np.zeros(iou_thresholds.shape)\n",
    "\n",
    "    # 101 recall points for interpolation\n",
    "    recall_vals = np.around(np.arange(0, 1.01, 0.01), decimals=2)    # [0:.01:1] R=101\n",
    "    \n",
    "    # idx of confidence scores in descending order\n",
    "    score_idx_list = np.argsort(pred_scores)[::-1]\n",
    "    \n",
    "    for i, iou_th in enumerate(iou_thresholds):\n",
    "        # get true positive values of detections\n",
    "        tp = np.array(eval_table[iou_th])\n",
    "        \n",
    "        # sort predictions based on confidence scores in descending order\n",
    "        tp = tp[score_idx_list]\n",
    "\n",
    "        # get false positives\n",
    "        fp = 1 - tp\n",
    "\n",
    "        # get accumulated tp and fp\n",
    "        acc_tp = np.cumsum(tp)\n",
    "        acc_fp = np.cumsum(fp)\n",
    "\n",
    "        # calculate precision and recall\n",
    "        precision = acc_tp / (acc_tp + acc_fp)\n",
    "        recall = acc_tp / total_num_gt_bbox\n",
    "    \n",
    "        \n",
    "        # get interpolated precision values on recall points with constraint that\n",
    "        # highest precision found for any recall level r' >= r\n",
    "        \n",
    "        # get maximum precision vals of reversed precision values\n",
    "        # reverse the result for recall vals\n",
    "        inter_p = np.maximum.accumulate(precision[::-1])[::-1]\n",
    "\n",
    "        # populate recall indices\n",
    "        recall_idx = np.searchsorted(recall, recall_vals, side=\"left\")\n",
    "\n",
    "        # get 101 point interpolated precision values\n",
    "        inter_p_101 = np.array([inter_p[r] if r < len(inter_p) else 0 for r in recall_idx])\n",
    "        \n",
    "        # compute average precision\n",
    "        ap = np.mean(inter_p_101)\n",
    "        \n",
    "        ap_vals[i] = ap\n",
    "        \n",
    "        # if iou is .5 or .75 \n",
    "        # print out AP50 and AP75 values\n",
    "        if iou_th == 0.5 or iou_th == 0.75:\n",
    "            print('AP{}: {}'.format(int(iou_th * 100), np.around(ap * 100, decimals=2)))\n",
    "    \n",
    "    # get mean of average precision values\n",
    "    ap = np.mean(ap_vals)\n",
    "    \n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZs1n3-mQzlu"
   },
   "outputs": [],
   "source": [
    "# compute Average Recall: 2 * area under the recall-iou curve    \n",
    "def calc_average_recall(eval_table, iou_vals, total_num_gt_bbox):\n",
    "    \n",
    "    # init recall values for different iou thresholds\n",
    "    recall = np.zeros(iou_vals.shape)\n",
    "    \n",
    "    for i, iou_val in enumerate(iou_vals):\n",
    "        # get true positive info of detections\n",
    "        tp = np.array(eval_table[iou_val])\n",
    "\n",
    "        # get total number of true positives\n",
    "        total_tp = np.sum(tp)\n",
    "        \n",
    "        # calculate recall\n",
    "        recall[i] = total_tp / total_num_gt_bbox\n",
    "    \n",
    "    # compute average recall -> 2 * area under the recall-iou curve\n",
    "    ar = 2 * np.trapz(recall, iou_vals)    # trapz - integrate y(x) along each 1d slice on the given axis\n",
    "    \n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gHQUf7tRH3e"
   },
   "outputs": [],
   "source": [
    "# confidence score threshold  \n",
    "score_threshold = 0.45\n",
    "\n",
    "# iou thresholds for average precision, AP -> [.50:.05:.95]\n",
    "ap_iou_thresholds = np.around(np.arange(0.5, 1, 0.05), decimals=2)\n",
    "\n",
    "# iou values for average recall, AR -> [.50:.1:1]\n",
    "ar_iou_thresholds = np.around(np.arange(0.5, 1.01, 0.1), decimals=2)\n",
    "\n",
    "# iou thresholds for AP and AR\n",
    "iou_thresholds = np.union1d(ap_iou_thresholds, ar_iou_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "819NX27fRI_B"
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # evaluation table\n",
    "    # in form of { iou_threshold: [ tp ] }\n",
    "    eval_dict = {}    \n",
    "                        \n",
    "    \n",
    "    # for different iou thresholds record tp info of each detection  \n",
    "    for iou_th in iou_thresholds:\n",
    "        eval_dict[iou_th] = []\n",
    "    \n",
    "    total_num_gt_bbox = 0\n",
    "    \n",
    "    all_pred_scores = [] # list of confidence scores of detections\n",
    "\n",
    "    # unmatched BBdt count as false positives and unmatched BBgt as false negatives\n",
    "    # matched BBdt with iou >= iou_thresold is considered true positives\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        \n",
    "        # for each image in test set\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            # get gorund-truth bounding boxes\n",
    "            gt_bboxes = label['boxes']\n",
    "            \n",
    "            # get detected bounding boxes and scores\n",
    "            # filter predictions based on a confidence score threshold\n",
    "            pred_bboxes = pred['boxes'][pred['scores'] > score_threshold]\n",
    "            pred_scores = pred['scores'][pred['scores'] > score_threshold] \n",
    "                        \n",
    "            # increase total number of ground truth bboxes\n",
    "            total_num_gt_bbox += len(gt_bboxes)\n",
    "            \n",
    "            # match bounding boxes for gt and dt and get ious\n",
    "            # in form of (dt_idx, gt_idx, iou)\n",
    "            iou_bboxes = get_iou_of_matched_bbox(gt_bboxes, pred_bboxes)\n",
    "            \n",
    "            # if no matched bbox is found, preds are fp\n",
    "            # record pred tp, pred score and continue on new image\n",
    "            if len(iou_bboxes) == 0: \n",
    "                tp = 0\n",
    "                for pred_score in pred_scores:\n",
    "                    for iou_th in iou_thresholds:\n",
    "                        eval_dict[iou_th].append(tp) \n",
    "                    all_pred_scores.append(pred_score)\n",
    "                continue\n",
    "                    \n",
    "            # get matched dt inds and ious\n",
    "            ious = iou_bboxes[:, 2]\n",
    "            matched_dt_inds = iou_bboxes[:, 0]\n",
    "            \n",
    "            # for each prediction\n",
    "            for j, (pred_bbox, pred_score) in enumerate(zip(pred_bboxes, pred_scores)):\n",
    "                # check if prediction is unmatched\n",
    "                fp = 0 if j in matched_dt_inds else 1\n",
    "                if fp == 1:\n",
    "                    # record tp value on each iou threshold\n",
    "                    for iou_th in iou_thresholds:\n",
    "                        eval_dict[iou_th].append(1 - fp)\n",
    "            \n",
    "                else:\n",
    "                    # get iou overlap for the detection\n",
    "                    iou, ious = ious[0], ious[1:]\n",
    "                    # for a set of different iou thresholds\n",
    "                    for iou_th in iou_thresholds:    \n",
    "                        # compute tp\n",
    "                        tp = 1 if iou >= iou_th else 0\n",
    "                        # record detection tp value\n",
    "                        eval_dict[iou_th].append(tp)\n",
    "                \n",
    "                # add confidence score of detection to the score list  \n",
    "                all_pred_scores.append(pred_score)\n",
    "                \n",
    "        \n",
    "    # get average precision\n",
    "    ap = calc_average_precision(eval_dict, ap_iou_thresholds, all_pred_scores, total_num_gt_bbox)\n",
    "    \n",
    "    # calculate average recall\n",
    "    ar = calc_average_recall(eval_dict, ar_iou_thresholds, total_num_gt_bbox)\n",
    "    \n",
    "    print('AP: {}'.format(np.around(ap * 100, decimals=2)))\n",
    "    print('AR: {}'.format(np.around(ar * 100, decimals=2)))\n",
    "\n",
    "    return ap, ar\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWic6uvNxkfE",
    "outputId": "ee032959-308b-4d40-c441-91c4f20d9bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP50: 16.11\n",
      "AP75: 7.42\n",
      "AP: 8.16\n",
      "AR: 12.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0816240605645483, 0.12844638949671772)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTmYuQaWF4Gx"
   },
   "source": [
    "## Explanations\n",
    "\n",
    " \n",
    "### Evaluation\n",
    "\n",
    "The evaluation happens by recording True Positive value of the each detected bounding box for the given IoU threshold. By forming an evaluation table at each IoU threshold, TP value and confidence score of every detection is recorded. The values needed to be calculated for Average Precision and Average Recall evaluation can be derived from TP value of detections, confidence score ranking and total number of ground truth bounding boxes.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "for each image in test set:<br>\n",
    "&emsp;get ground truth bounding boxes<br>\n",
    "&emsp;get detected bounding boxes<br>\n",
    "&emsp;match dt with gt bounding box and get IoU values<br>\n",
    "&emsp;for each detected bounding box:<br>\n",
    "&emsp;&emsp;for each IoU threshold:<br>\n",
    "&emsp;&emsp;&emsp;if detection is unmatched(FP) record 0<br>\n",
    "&emsp;&emsp;&emsp;if detection is matched but IoU overlap is less than IoU threshold(FP) record 0<br> \n",
    "&emsp;&emsp;&emsp;if detection is matched and IoU overlap is greater than or equal to IoU threshold(TP) record 1<br>\n",
    "&emsp;&emsp;record confidence score<br>\n",
    "calculate Average Precision with recorded values<br>\n",
    "calculate Average Recall with recorded values\n",
    "\n",
    "---\n",
    "\n",
    "Above implementation considers only bounding boxes for evaluation and omits classification.\n",
    "\n",
    "#### Some Helpful Resources\n",
    "\n",
    "- What is Average Precision in Object Detection & Localization Algorithms and how to calculate it?, https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b\n",
    "- A Survey on Performance Metrics for Object-Detection Algorithms, https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/paper_survey_on_performance_metrics_for_object_detection_algorithms.pdf\n",
    "- Precision, Recall & Mean Average Precision for Object Detection Playlist, https://www.youtube.com/playlist?list=PL1GQaVhO4f_jE5pnXU_Q4MSrIQx4wpFLM\n",
    "- COCO Dataset Detection Evaluation, https://cocodataset.org/#detection-eval\n",
    "- An Introduction to Evaluation Metrics for Object Detection Blog Post, https://blog.zenggyu.com/en/post/2018-12-16/an-introduction-to-evaluation-metrics-for-object-detection/#fn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQ6Rp6WbF6tl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "31bc2eb2e0104ee8ba800084ccd5d8be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ccc05ac95b746d092f2905b063ec717": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7feb973bab3e4f019296d64d149aa037": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "815e7bd65b4b4494bb5256162c9aa174": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d82155fd70184a3194e4eb5d95c857ee",
      "placeholder": "​",
      "style": "IPY_MODEL_a119d70f58984c28b2a725f618d46166",
      "value": "100%"
     }
    },
    "911877063cdf4c2d907270ac346da38f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_815e7bd65b4b4494bb5256162c9aa174",
       "IPY_MODEL_969b520715b846498fbacedd9e89f5b6",
       "IPY_MODEL_ccbc9f5f3b4e404ab9da76254d016178"
      ],
      "layout": "IPY_MODEL_4ccc05ac95b746d092f2905b063ec717"
     }
    },
    "969b520715b846498fbacedd9e89f5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7feb973bab3e4f019296d64d149aa037",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb59988ee54b4b7aad5687b79695e1f7",
      "value": 46830571
     }
    },
    "9ee43af26652482cbd0f390a84a46417": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a119d70f58984c28b2a725f618d46166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccbc9f5f3b4e404ab9da76254d016178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ee43af26652482cbd0f390a84a46417",
      "placeholder": "​",
      "style": "IPY_MODEL_31bc2eb2e0104ee8ba800084ccd5d8be",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 65.0MB/s]"
     }
    },
    "d82155fd70184a3194e4eb5d95c857ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb59988ee54b4b7aad5687b79695e1f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
